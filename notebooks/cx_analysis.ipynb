{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Experience Optimization Analysis - StyleForward Retail\n",
    "\n",
    "## Overview\n",
    "This notebook analyzes customer touchpoints, transactions, and behavioral data to identify friction points in the customer experience journey. We'll uncover hidden patterns that reveal why StyleForward is losing $31.2M annually despite apparently healthy conversion metrics.\n",
    "\n",
    "### Key Questions to Answer:\n",
    "1. Where are the critical friction points causing customer abandonment?\n",
    "2. How does experience differ between mobile and desktop users?\n",
    "3. Why do social media channels show 0% conversion despite heavy investment?\n",
    "4. What patterns exist in successful vs failed customer journeys?\n",
    "5. How can we quantify the revenue impact of experience gaps?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Data Ingestion\n",
    "Loading all necessary datasets for the customer experience analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# Set style for visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all CSV files\n",
    "try:\n",
    "    # Core datasets for CX analysis\n",
    "    customers = pd.read_csv('https://raw.githubusercontent.com/jimcody2014/eic/refs/heads/main/customers.csv')\n",
    "    touchpoints = pd.read_csv('https://raw.githubusercontent.com/jimcody2014/eic/refs/heads/main/customer_touchpoints.csv')\n",
    "    stores = pd.read_csv('https://raw.githubusercontent.com/jimcody2014/eic/refs/heads/main/stores.csv')\n",
    "    products = pd.read_csv('https://raw.githubusercontent.com/jimcody2014/eic/refs/heads/main/products.csv')\n",
    "\n",
    "    print(\"All datasets loaded successfully!\")\n",
    "    print(\"\\nDataset shapes:\")\n",
    "    print(f\"  Customers: {customers.shape}\")    \n",
    "    print(f\"  Customer Touchpoints: {touchpoints.shape}\")\n",
    " \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading files: {e}\")\n",
    "    print(\"Please ensure all CSV files are in the current directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "touchpoints.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Exercise (5 minutes)\n",
    "\n",
    "1. Read in the transactions data. Name the df *transactions*.\n",
    "2. Identify the number of rows and columns\n",
    "3. Display the first **10** row\n",
    "\n",
    "https://raw.githubusercontent.com/jimcody2014/eic/refs/heads/main/transactions.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add customer transactions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transactionss shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transactions first 10 rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Data Cleaning\n",
    "Identifying and resolving data quality issues including duplicates, missing values, incorrect data types, and inconsistencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for duplicates\n",
    "# .duplicated test to see if the entire row is a duplicate\n",
    "\n",
    "customers.loc[customers.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.drop_duplicates(keep = 'first', inplace = True) \n",
    "\n",
    "# keep - which duplicate to keep, default is none!\n",
    "\n",
    "customers.loc[customers.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows in multiple dataset\n",
    "# Example of automating the process\n",
    "\n",
    "datasets = {\n",
    "    'customers': customers,\n",
    "    'stores': stores,\n",
    "    'products': products\n",
    "}\n",
    "\n",
    "print(\"Checking for duplicate rows:\")\n",
    "print(\"=\"*40)\n",
    "for name, df in datasets.items():\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"{name}: {duplicates} duplicate rows\")\n",
    "    \n",
    "    # Check for duplicates on key columns\n",
    "    if name == 'stores':\n",
    "        dup_store_ids = df['store_id'].duplicated().sum()\n",
    "        print(f\"  - Duplicate store_ids: {dup_store_ids}\")\n",
    "    elif name == 'transactions':\n",
    "        dup_transaction_ids = df['transaction_id'].duplicated().sum()\n",
    "        print(f\"  - Duplicate transaction_ids: {dup_transaction_ids}\")\n",
    "    elif name == 'customers':\n",
    "        dup_customer_ids = df['customer_id'].duplicated().sum()\n",
    "        print(f\"  - Duplicate customer_ids: {dup_customer_ids}\")\n",
    "\n",
    "# Remove any duplicates found\n",
    "for name, df in datasets.items():\n",
    "    before = len(df)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    after = len(df)\n",
    "    if before != after:\n",
    "        print(f\"\\nRemoved {before-after} duplicates from {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find and Handle Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just listing the columns and how many rows \n",
    "# for each have a missing value.\n",
    "\n",
    "customers.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_null = customers.isna().mean().round(4) * 100\n",
    "\n",
    "customers_null.sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting missing values\n",
    "\n",
    "sns.heatmap(customers.isnull(), cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = customers['lifetime_value'].describe()\n",
    "b = customers['lifetime_value'].median()\n",
    "c = customers['lifetime_value'].mode()\n",
    "print(a)\n",
    "print()\n",
    "print(f'Median: {b}')\n",
    "print()\n",
    "print(f'Mode: {c}')\n",
    "\n",
    "#if len(c) == 1:\n",
    "#    print(f'Mode: {c.iloc[0]}')\n",
    "#else:\n",
    "#    print(f'Multiple modes ({len(c)} values). Example first mode: {c.iloc[0]}')\n",
    "#    print('All modes:', c.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the lifetime value with the mean for the column\n",
    "\n",
    "customers.lifetime_value.fillna(customers.lifetime_value.mean(),inplace=True)\n",
    "\n",
    "# Check missing values in customers\n",
    "print(\"\\nMissing Values Analysis - Customers\")\n",
    "print(\"=\"*50)\n",
    "missing_customers = customers.isnull().sum()\n",
    "missing_pct_cust = (missing_customers / len(customers)) * 100\n",
    "missing_cust_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_customers,\n",
    "    'Percentage': missing_pct_cust\n",
    "})\n",
    "print(missing_cust_df[missing_cust_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in touchpoints\n",
    "print(\"Missing Values Analysis - touchpoints\")\n",
    "print(\"=\"*50)\n",
    "missing_touchpoints = touchpoints.isnull().sum()\n",
    "missing_pct = (missing_touchpoints / len(touchpoints)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_touchpoints,\n",
    "    'Percentage': missing_pct\n",
    "})\n",
    "print(missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values strategically\n",
    "print(\"Handling Missing Values:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# For touchpoints dataset\n",
    "# Fill missing campaign_id with -1 (indicates organic/direct traffic)\n",
    "touchpoints['campaign_id'].fillna(-1, inplace=True)\n",
    "print(\"Filled missing campaign_id with -1 (organic traffic indicator)\")\n",
    "\n",
    "# Fill missing store_id with -1 (indicates online interaction)\n",
    "touchpoints['store_id'].fillna(-1, inplace=True)\n",
    "print(\"Filled missing store_id with -1 (online interaction)\")\n",
    "\n",
    "# Fill missing transaction_id with -1 (indicates no conversion)\n",
    "touchpoints['transaction_id'].fillna(-1, inplace=True)\n",
    "print(\"Filled missing transaction_id with -1 (no conversion)\")\n",
    "\n",
    "# For customers dataset\n",
    "# Fill missing state with TX\n",
    "customers['state'].fillna('TX', inplace=True)\n",
    "print(\"Filled missing state in customers with TX\")\n",
    "\n",
    "\n",
    "print(\"\\nMissing values handled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Exercise (10 minutes)\n",
    "\n",
    "1. Does transactions have any duplicates?\n",
    "2. Does it have any missing values? Which variables should you populate with imputed data?\n",
    "3. Populate missing shipping amounts with the mean of the column.\n",
    "4. Populate missing store ids with -1 to indicate an online.\n",
    "5. Populate missing promo codes with 'None'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check duplicates in transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values in transactions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate missing shipping_value rows with the mean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing store_id with -1 (online transaction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing promo_code with 'NONE'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Type Corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current data types\n",
    "touchpoints.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert store_id from float to string (it's actually a store identifier, not a numeric value)\n",
    "\n",
    "# Convert to categorical\n",
    "touchpoints['store_id'] = touchpoints['store_id'].astype('string')\n",
    "\n",
    "print(\"Converted store_id to string type\")\n",
    "print(f\"New data type: {touchpoints['store_id'].dtype}\")\n",
    "\n",
    "# Convert campaign_id to string as well\n",
    "touchpoints['campaign_id'] = touchpoints['campaign_id'].astype('string')\n",
    "print(\"Converted campaign_id to string type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to numeric example\n",
    "# vaccines['series_complete_pop_pct'] = pd.to_numeric(vaccines['series_complete_pop_pct']).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename Columns for Clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for better clarity\n",
    "print(\"Renaming columns for clarity...\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Rename in touchpoints\n",
    "touchpoints.rename(columns={\n",
    "    'converted_flag': 'conversion_status',\n",
    "    'touchpoint_timestamp': 'interaction_datetime'\n",
    "}, inplace=True)\n",
    "\n",
    "print(\" Renamed touchpoints columns:\")\n",
    "print(\"  - converted_flag → conversion_status\")\n",
    "print(\"  - touchpoint_timestamp → interaction_datetime\")\n",
    "\n",
    "print(\"\\nNew column names:\")\n",
    "print(f\"Touchpoints: {list(touchpoints.columns[:5])}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Exercise (5 minutes)\n",
    "\n",
    "1. Change tranactions store_id to a category.\n",
    "2. Rename return_flag to was_returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current data types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change store_id to a category data type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename in transactions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Data Analysis and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine touchpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze categorical columns: channel, device_type, touchpoint_type\n",
    "categorical_cols = ['channel', 'device_type', 'touchpoint_type']\n",
    "\n",
    "print(\"Categorical Data Analysis\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col.upper()}:\")\n",
    "    print(f\"  Unique values: {touchpoints[col].nunique()}\")\n",
    "    print(f\"  Unique list: {touchpoints[col].unique().tolist()}\")\n",
    "    print(f\"\\n  Value counts:\")\n",
    "    print(touchpoints[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar charts for categorical variables\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Channel distribution\n",
    "channel_counts = touchpoints['channel'].value_counts()\n",
    "axes[0].bar(channel_counts.index, channel_counts.values, color='steelblue')\n",
    "axes[0].set_title('Distribution of Touchpoints by Channel', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Channel')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Device type distribution\n",
    "device_counts = touchpoints['device_type'].value_counts()\n",
    "axes[1].bar(device_counts.index, device_counts.values, color='coral')\n",
    "axes[1].set_title('Distribution by Device Type', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Device Type')\n",
    "axes[1].set_ylabel('Count')\n",
    "\n",
    "# Touchpoint type distribution\n",
    "touchpoint_counts = touchpoints['touchpoint_type'].value_counts()\n",
    "axes[2].bar(touchpoint_counts.index, touchpoint_counts.values, color='seagreen')\n",
    "axes[2].set_title('Distribution by Touchpoint Type', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Touchpoint Type')\n",
    "axes[2].set_ylabel('Count')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Charts for customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='gender', data=customers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix Data Inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change/Fix some of the data values\n",
    "\n",
    "customers['gender'] = customers['gender'].replace({'M':'Male', 'F':'Female'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inconsistent capitalization\n",
    "# Apply a function along an axis of the DataFrame.\n",
    "\n",
    "customers['gender'] = customers['gender'].apply(lambda x:x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = customers.loc[customers.gender == 'unknown/invalid','gender']\n",
    "y = customers.loc[customers.gender == '0','gender']\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers['gender'] = customers['gender'].replace({'0':'female'})\n",
    "sns.countplot(x='gender', data=customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for inconsistencies in categorical data\n",
    "print(\"Checking for Data Inconsistencies\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check channel values for inconsistencies\n",
    "print(\"\\nChannel values (checking for case inconsistencies):\")\n",
    "channels = touchpoints['channel'].str.lower().value_counts()\n",
    "print(channels)\n",
    "\n",
    "# Standardize channel names (ensure consistent casing)\n",
    "touchpoints['channel'] = touchpoints['channel'].str.title()\n",
    "print(\"\\n✓ Standardized channel names to Title Case\")\n",
    "\n",
    "# Check for inconsistent device type values\n",
    "print(\"\\nDevice Type values:\")\n",
    "print(touchpoints['device_type'].value_counts())\n",
    "\n",
    "# Fix any None/NaN values in device_type\n",
    "touchpoints['device_type'].fillna('Unknown', inplace=True)\n",
    "print(\"✓ Filled missing device_type with 'Unknown'\")\n",
    "\n",
    "# Check referrer_source for cleanup\n",
    "print(\"\\nReferrer source values (sample):\")\n",
    "print(touchpoints['referrer_source'].value_counts().head())\n",
    "\n",
    "# Standardize referrer_source\n",
    "touchpoints['referrer_source'] = touchpoints['referrer_source'].str.lower().str.strip()\n",
    "print(\"✓ Standardized referrer_source to lowercase and removed whitespace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric Data Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze numeric columns\n",
    "numeric_cols = ['pages_viewed', 'cart_additions', 'cart_value']\n",
    "\n",
    "print(\"Numeric Data Statistics\")\n",
    "print(\"=\"*50)\n",
    "print(touchpoints[numeric_cols].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's talk about histograms for a minute!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the bin size to get a picture that is helpful.\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_theme(style=\"ticks\")\n",
    "\n",
    "diamonds = sns.load_dataset(\"diamonds\")\n",
    "\n",
    "f, ax = plt.subplots(figsize=(7, 5))\n",
    "sns.despine(f)\n",
    "sns.histplot(data=diamonds, x='price', binwidth = 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histgrams for project data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histograms for numeric columns\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Pages viewed histogram\n",
    "axes[0].hist(touchpoints['pages_viewed'].dropna(), bins=15, color='skyblue', edgecolor='black')\n",
    "axes[0].set_title('Distribution of Pages Viewed', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Pages Viewed')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].axvline(touchpoints['pages_viewed'].mean(), color='red', linestyle='--', label=f'Mean: {touchpoints[\"pages_viewed\"].mean():.1f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Cart additions histogram\n",
    "axes[1].hist(touchpoints['cart_additions'], bins=10, color='lightcoral', edgecolor='black')\n",
    "axes[1].set_title('Distribution of Cart Additions', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Cart Additions')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].axvline(touchpoints['cart_additions'].mean(), color='red', linestyle='--', label=f'Mean: {touchpoints[\"cart_additions\"].mean():.1f}')\n",
    "axes[1].legend()\n",
    "\n",
    "# Cart value histogram (excluding zeros)\n",
    "cart_values_nonzero = touchpoints[touchpoints['cart_value'] > 0]['cart_value']\n",
    "axes[2].hist(cart_values_nonzero, bins=15, color='lightgreen', edgecolor='black')\n",
    "axes[2].set_title('Distribution of Cart Value (Non-zero)', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Cart Value ($)')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "axes[2].axvline(cart_values_nonzero.mean(), color='red', linestyle='--', label=f'Mean: ${cart_values_nonzero.mean():.2f}')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplots for project data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boxplots for numeric columns\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Pages viewed boxplot\n",
    "axes[0].boxplot(touchpoints['pages_viewed'].dropna(), vert=True)\n",
    "axes[0].set_title('Pages Viewed Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Pages Viewed')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Cart additions boxplot\n",
    "axes[1].boxplot(touchpoints['cart_additions'].dropna(), vert=True)\n",
    "axes[1].set_title('Cart Additions Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Cart Additions')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Cart value boxplot (non-zero)\n",
    "axes[2].boxplot(cart_values_nonzero, vert=True)\n",
    "axes[2].set_title('Cart Value Distribution (Non-zero)', fontsize=12, fontweight='bold')\n",
    "axes[2].set_ylabel('Cart Value ($)')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify outliers\n",
    "Q1 = touchpoints['cart_value'].quantile(0.25)\n",
    "Q3 = touchpoints['cart_value'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = touchpoints[(touchpoints['cart_value'] < Q1 - 1.5*IQR) | (touchpoints['cart_value'] > Q3 + 1.5*IQR)]\n",
    "print(f\"\\nFound {len(outliers)} outliers in cart_value (using IQR method)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pair plot for numeric variables\n",
    "numeric_data = touchpoints[['pages_viewed', 'cart_additions', 'cart_value', 'conversion_status']].copy()\n",
    "numeric_data['conversion_status'] = numeric_data['conversion_status'].astype(int)\n",
    "\n",
    "# Create pairplot\n",
    "fig = sns.pairplot(numeric_data, hue='conversion_status', \n",
    "                   palette={0: 'coral', 1: 'seagreen'},\n",
    "                   diag_kind='kde',\n",
    "                   plot_kws={'alpha': 0.6})\n",
    "fig.fig.suptitle('Pair Plot of Numeric Variables by Conversion Status', y=1.02, fontsize=14, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(\"Pair plot shows relationships between:\")\n",
    "print(\"- Pages viewed\")\n",
    "print(\"- Cart additions\")\n",
    "print(\"- Cart value\")\n",
    "print(\"- Colored by conversion status (0=No conversion, 1=Converted)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Exercise (30 minutes)\n",
    "\n",
    "1. Explore the transactions data.\n",
    "2. Look at the categorical data.\n",
    "3. Look at the numeric data.\n",
    "4. Make sure every column is examined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Just a function to organize the data\n",
    "\n",
    "def split_columns(df: pd.DataFrame) -> dict:\n",
    "    out = {\n",
    "        \"numeric\":   df.select_dtypes(include=[np.number]).columns.tolist(),\n",
    "        \"integer\":   df.select_dtypes(include=[np.integer]).columns.tolist(),\n",
    "        \"float\":     df.select_dtypes(include=[np.floating]).columns.tolist(),\n",
    "        \"boolean\":   df.select_dtypes(include=['bool', 'boolean']).columns.tolist(),\n",
    "        \"object\":    df.select_dtypes(include=['object', 'string']).columns.tolist(),\n",
    "        \"category\":  df.select_dtypes(include=['category']).columns.tolist(),\n",
    "        \"datetime\":  df.select_dtypes(include=['datetime', 'datetimetz']).columns.tolist(),\n",
    "    }\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the df name that the function is applied to\n",
    "\n",
    "#########\n",
    "# After transactions is added to the notebook.  Uncomment the lines below.\n",
    "#########\n",
    "\n",
    "cols = split_columns(transactions)\n",
    "numeric_cols = cols['numeric']\n",
    "integer_cols = cols['integer']\n",
    "float_cols = cols['float']\n",
    "boolean_cols = cols['boolean']\n",
    "object_cols = cols['object']\n",
    "category_cols = cols['category']\n",
    "datetime_cols = cols['datetime']\n",
    "\n",
    "# print(f'Numeric: {numeric_cols}')\n",
    "# print(f'Integer: {integer_cols}')\n",
    "# print(f'Float: {float_cols}')\n",
    "# print(f'Boolean: {boolean_cols}')\n",
    "# print(f'Object: {object_cols}')\n",
    "# print(f'Category: {category_cols}')\n",
    "# print(f'Datetime: {datetime_cols}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercie Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Data Transformation\n",
    "Creating derived features and transforming data to enable deeper analysis of customer experience patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation Planning\n",
    "\n",
    "To uncover the hidden friction points in the customer journey, we need to create several derived features:\n",
    "\n",
    "1. **Time-based features**: Extract hour, day of week, and time of day from timestamps to identify temporal patterns\n",
    "2. **Journey complexity metrics**: Calculate touchpoints per customer, channels used, devices used\n",
    "3. **Conversion funnel stages**: Map touchpoints to funnel stages (awareness, consideration, purchase)\n",
    "4. **Conversion and Abandoment indicators**: Flags abandoned carts and the abandoment value\n",
    "5. **Friction indicators**: Calculate abandonment rates, time between touchpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Parse and extract time-based features\n",
    "print(\"Creating Time-Based Features\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Parse the interaction datetime\n",
    "touchpoints['interaction_datetime_parsed'] = pd.to_datetime(touchpoints['interaction_datetime'], format='%m/%d/%y %H:%M', errors='coerce')\n",
    "\n",
    "# Extract time components\n",
    "touchpoints['hour'] = touchpoints['interaction_datetime_parsed'].dt.hour\n",
    "touchpoints['day_of_week'] = touchpoints['interaction_datetime_parsed'].dt.dayofweek\n",
    "touchpoints['day_name'] = touchpoints['interaction_datetime_parsed'].dt.day_name()\n",
    "\n",
    "# Create time of day categories\n",
    "def categorize_time_of_day(hour):\n",
    "    if pd.isna(hour):\n",
    "        return 'Unknown'\n",
    "    elif 6 <= hour < 12:\n",
    "        return 'Morning'\n",
    "    elif 12 <= hour < 17:\n",
    "        return 'Afternoon'\n",
    "    elif 17 <= hour < 21:\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return 'Night'\n",
    "\n",
    "touchpoints['time_of_day'] = touchpoints['hour'].apply(categorize_time_of_day)\n",
    "\n",
    "print(f\"Created time-based features: hour, day_of_week, day_name, time_of_day\")\n",
    "print(f\"\\nTime of day distribution:\")\n",
    "print(touchpoints['time_of_day'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "touchpoints.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Calculate journey complexity metrics\n",
    "print(\"Calculating Journey Complexity Metrics\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Customer journey aggregations\n",
    "customer_journey_metrics = touchpoints.groupby('customer_id').agg({\n",
    "    'touchpoint_id': 'count',\n",
    "    'channel': lambda x: x.nunique(),\n",
    "    'device_type': lambda x: x.nunique(),\n",
    "    'conversion_status': lambda x: x.any(),\n",
    "    'cart_value': 'sum',\n",
    "    'pages_viewed': 'sum',\n",
    "    'cart_additions': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "customer_journey_metrics.columns = ['customer_id', 'total_touchpoints', 'channels_used', \n",
    "                                    'devices_used', 'converted', 'total_cart_value',\n",
    "                                    'total_pages_viewed', 'total_cart_additions']\n",
    "\n",
    "# Add flags for multi-channel and multi-device\n",
    "customer_journey_metrics['is_multichannel'] = customer_journey_metrics['channels_used'] > 1\n",
    "customer_journey_metrics['is_multidevice'] = customer_journey_metrics['devices_used'] > 1\n",
    "\n",
    "print(f\"Created journey metrics for {len(customer_journey_metrics)} customers\")\n",
    "print(f\"\\nJourney complexity summary:\")\n",
    "print(f\"  Multi-channel customers: {customer_journey_metrics['is_multichannel'].sum()} ({customer_journey_metrics['is_multichannel'].mean()*100:.1f}%)\")\n",
    "print(f\"  Multi-device customers: {customer_journey_metrics['is_multidevice'].sum()} ({customer_journey_metrics['is_multidevice'].mean()*100:.1f}%)\")\n",
    "print(f\"  Average touchpoints per customer: {customer_journey_metrics['total_touchpoints'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Map touchpoints to funnel stages\n",
    "print(\"Mapping Touchpoints to Funnel Stages\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def assign_funnel_stage(row):\n",
    "    \"\"\"Assign each touchpoint to a funnel stage based on behavior\"\"\"\n",
    "    if row['conversion_status'] == True:\n",
    "        return 'Purchase'\n",
    "    elif row['cart_additions'] > 0:\n",
    "        return 'Consideration'\n",
    "    else:\n",
    "        return 'Awareness'\n",
    "\n",
    "touchpoints['funnel_stage'] = touchpoints.apply(assign_funnel_stage, axis=1)\n",
    "\n",
    "print(\"Assigned funnel stages to all touchpoints\")\n",
    "print(\"\\nFunnel stage distribution:\")\n",
    "funnel_dist = touchpoints['funnel_stage'].value_counts()\n",
    "for stage in ['Awareness', 'Consideration', 'Purchase']:\n",
    "    count = funnel_dist.get(stage, 0)\n",
    "    pct = (count / len(touchpoints)) * 100\n",
    "    print(f\"  {stage}: {count} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create conversion and abandonment indicators\n",
    "print(\"Creating Conversion and Abandonment Indicators\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Flag abandoned carts\n",
    "touchpoints['cart_abandoned'] = (touchpoints['cart_additions'] > 0) & (touchpoints['conversion_status'] == False)\n",
    "\n",
    "# Calculate abandonment value\n",
    "touchpoints['abandonment_value'] = touchpoints.apply(\n",
    "    lambda x: x['cart_value'] if x['cart_abandoned'] else 0, axis=1\n",
    ")\n",
    "\n",
    "total_abandoned_value = touchpoints['abandonment_value'].sum()\n",
    "print(f\"✓ Identified cart abandonment patterns\")\n",
    "print(f\"\\nAbandonment metrics:\")\n",
    "print(f\"  Abandoned carts: {touchpoints['cart_abandoned'].sum()}\")\n",
    "print(f\"  Total abandoned value: ${total_abandoned_value:,.2f}\")\n",
    "print(f\"  Average abandoned cart value: ${touchpoints[touchpoints['cart_abandoned']]['cart_value'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Create customer value segments\n",
    "print(\"Creating Customer Value Segments\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Merge customer data with journey metrics\n",
    "customers_enhanced = customers.merge(customer_journey_metrics, on='customer_id', how='left')\n",
    "\n",
    "# Create value tiers based on lifetime value\n",
    "def assign_value_tier(ltv):\n",
    "    if pd.isna(ltv) or ltv == 0:\n",
    "        return 'No Value'\n",
    "    elif ltv < 500:\n",
    "        return 'Low Value'\n",
    "    elif ltv < 1500:\n",
    "        return 'Medium Value'\n",
    "    else:\n",
    "        return 'High Value'\n",
    "\n",
    "customers_enhanced['value_tier'] = customers_enhanced['lifetime_value'].apply(assign_value_tier)\n",
    "\n",
    "print(\"✓ Created customer value tiers\")\n",
    "print(\"\\nValue tier distribution:\")\n",
    "print(customers_enhanced['value_tier'].value_counts())\n",
    "\n",
    "# Calculate metrics by value tier\n",
    "print(\"\\nMetrics by value tier:\")\n",
    "tier_metrics = customers_enhanced.groupby('value_tier').agg({\n",
    "    'total_touchpoints': 'mean',\n",
    "    'is_multichannel': 'mean',\n",
    "    'is_multidevice': 'mean'\n",
    "})\n",
    "print(tier_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Create channel attribution features\n",
    "print(\"Creating Channel Attribution Features\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Identify first and last touch channels for each customer\n",
    "customer_touchpoints = touchpoints.sort_values(['customer_id', 'interaction_datetime_parsed'])\n",
    "\n",
    "first_touch = customer_touchpoints.groupby('customer_id').first()[['channel']].rename(columns={'channel': 'first_touch_channel'})\n",
    "last_touch = customer_touchpoints.groupby('customer_id').last()[['channel']].rename(columns={'channel': 'last_touch_channel'})\n",
    "\n",
    "# Merge attribution data\n",
    "attribution_df = first_touch.merge(last_touch, on='customer_id')\n",
    "attribution_df['channel_switch'] = attribution_df['first_touch_channel'] != attribution_df['last_touch_channel']\n",
    "\n",
    "print(f\"Created attribution features\")\n",
    "print(f\"\\nChannel switching:\")\n",
    "print(f\"  Customers who switched channels: {attribution_df['channel_switch'].sum()} ({attribution_df['channel_switch'].mean()*100:.1f}%)\")\n",
    "\n",
    "# Most common channel paths\n",
    "print(\"\\nTop channel paths (first → last):\")\n",
    "channel_paths = attribution_df[attribution_df['channel_switch']].groupby(['first_touch_channel', 'last_touch_channel']).size().sort_values(ascending=False).head()\n",
    "print(channel_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Exercise (30 minutes)\n",
    "\n",
    "Every team project will need to use the tranasctions data.  Based on the needs of your team project, work with your partner(s) to determine if there are any transformations of transactions that will be required.\n",
    "\n",
    "Code the requirement.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Descriptive Data Analysis\n",
    "Comprehensive analysis of customer experience metrics to understand current state performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Plan for Descriptive Analysis\n",
    "\n",
    "In this section, we will:\n",
    "1. **Calculate overall conversion metrics** - Understand baseline performance\n",
    "2. **Analyze channel performance** - Identify which channels drive conversions\n",
    "3. **Examine device-specific behaviors** - Quantify mobile vs desktop experience gaps\n",
    "4. **Study customer segment patterns** - Understand how different segments behave\n",
    "5. **Measure cart abandonment impact** - Quantify lost revenue opportunities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Overall Conversion Metrics\n",
    "print(\"=\"*60)\n",
    "print(\"OVERALL CONVERSION METRICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "total_sessions = len(touchpoints)\n",
    "converted_sessions = touchpoints['conversion_status'].sum()\n",
    "conversion_rate = (converted_sessions / total_sessions) * 100\n",
    "\n",
    "sessions_with_cart = len(touchpoints[touchpoints['cart_additions'] > 0])\n",
    "cart_to_purchase = touchpoints[touchpoints['cart_additions'] > 0]['conversion_status'].sum()\n",
    "cart_conversion_rate = (cart_to_purchase / sessions_with_cart) * 100 if sessions_with_cart > 0 else 0\n",
    "\n",
    "print(f\"Total Touchpoints: {total_sessions}\")\n",
    "print(f\"Converted Touchpoints: {converted_sessions}\")\n",
    "print(f\"Overall Conversion Rate: {conversion_rate:.1f}%\")\n",
    "print(f\"\\nCart Metrics:\")\n",
    "print(f\"  Sessions with Cart Additions: {sessions_with_cart}\")\n",
    "print(f\"  Cart-to-Purchase Conversion: {cart_conversion_rate:.1f}%\")\n",
    "print(f\"  Cart Abandonment Rate: {100 - cart_conversion_rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Channel Performance Analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CHANNEL PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "channel_metrics = touchpoints.groupby('channel').agg({\n",
    "    'touchpoint_id': 'count',\n",
    "    'conversion_status': ['sum', 'mean'],\n",
    "    'cart_value': 'mean',\n",
    "    'pages_viewed': 'mean',\n",
    "    'cart_additions': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "channel_metrics.columns = ['Touchpoints', 'Conversions', 'Conv_Rate', 'Avg_Cart_Value', 'Avg_Pages', 'Avg_Cart_Items']\n",
    "channel_metrics['Conv_Rate'] = (channel_metrics['Conv_Rate'] * 100).round(1)\n",
    "channel_metrics = channel_metrics.sort_values('Conv_Rate', ascending=False)\n",
    "\n",
    "print(channel_metrics)\n",
    "\n",
    "# Identify problem channels\n",
    "zero_conv_channels = channel_metrics[channel_metrics['Conv_Rate'] == 0].index.tolist()\n",
    "if zero_conv_channels:\n",
    "    print(f\"\\nCRITICAL: Channels with 0% conversion: {', '.join(zero_conv_channels)}\")\n",
    "    print(\"   These channels are consuming marketing spend with no return!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Device-Specific Behavior Analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DEVICE-SPECIFIC BEHAVIOR ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "device_metrics = touchpoints.groupby('device_type').agg({\n",
    "    'touchpoint_id': 'count',\n",
    "    'conversion_status': ['sum', 'mean'],\n",
    "    'cart_value': 'mean',\n",
    "    'pages_viewed': 'mean',\n",
    "    'cart_additions': 'mean',\n",
    "    'cart_abandoned': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "device_metrics.columns = ['Sessions', 'Conversions', 'Conv_Rate', 'Avg_Cart_Value', \n",
    "                          'Avg_Pages', 'Avg_Cart_Items', 'Abandoned_Carts']\n",
    "device_metrics['Conv_Rate'] = (device_metrics['Conv_Rate'] * 100).round(1)\n",
    "\n",
    "print(device_metrics)\n",
    "\n",
    "# Calculate mobile vs desktop gap\n",
    "if 'Mobile' in device_metrics.index and 'Desktop' in device_metrics.index:\n",
    "    mobile_conv = device_metrics.loc['Mobile', 'Conv_Rate']\n",
    "    desktop_conv = device_metrics.loc['Desktop', 'Conv_Rate']\n",
    "    conv_gap = desktop_conv - mobile_conv\n",
    "    \n",
    "    print(f\"\\nMOBILE EXPERIENCE GAP:\")\n",
    "    print(f\"   Desktop Conversion: {desktop_conv:.1f}%\")\n",
    "    print(f\"   Mobile Conversion: {mobile_conv:.1f}%\")\n",
    "    print(f\"   Gap: -{conv_gap:.1f}% (Mobile underperforms by {conv_gap:.1f} percentage points)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Customer Segment Analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CUSTOMER SEGMENT ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "segment_metrics = customers_enhanced.groupby('customer_segment').agg({\n",
    "    'customer_id': 'count',\n",
    "    'lifetime_value': 'mean',\n",
    "    'total_touchpoints': 'mean',\n",
    "    'is_multichannel': lambda x: (x == True).sum(),\n",
    "    'is_multidevice': lambda x: (x == True).sum()\n",
    "}).round(2)\n",
    "\n",
    "segment_metrics.columns = ['Customers', 'Avg_LTV', 'Avg_Touchpoints', 'Multichannel', 'Multidevice']\n",
    "segment_metrics = segment_metrics.sort_values('Avg_LTV', ascending=False)\n",
    "\n",
    "print(segment_metrics)\n",
    "\n",
    "# Identify high-value segments\n",
    "high_value_segments = segment_metrics[segment_metrics['Avg_LTV'] > segment_metrics['Avg_LTV'].mean()].index.tolist()\n",
    "print(f\"\\nHigh-value segments (above average LTV): {', '.join(high_value_segments)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Temporal Pattern Analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEMPORAL PATTERN ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Time of day analysis\n",
    "time_metrics = touchpoints.groupby('time_of_day').agg({\n",
    "    'conversion_status': ['count', 'sum', 'mean'],\n",
    "    'cart_value': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "time_metrics.columns = ['Sessions', 'Conversions', 'Conv_Rate', 'Avg_Cart_Value']\n",
    "time_metrics['Conv_Rate'] = (time_metrics['Conv_Rate'] * 100).round(1)\n",
    "\n",
    "# Reorder by typical day flow\n",
    "time_order = ['Morning', 'Afternoon', 'Evening', 'Night', 'Unknown']\n",
    "time_metrics = time_metrics.reindex([t for t in time_order if t in time_metrics.index])\n",
    "\n",
    "print(\"Performance by Time of Day:\")\n",
    "print(time_metrics)\n",
    "\n",
    "# Hour-level analysis for patterns\n",
    "hour_conversions = touchpoints.groupby('hour')['conversion_status'].agg(['count', 'sum', 'mean'])\n",
    "hour_conversions['conv_rate'] = hour_conversions['mean'] * 100\n",
    "\n",
    "best_hours = hour_conversions.nlargest(3, 'conv_rate')\n",
    "worst_hours = hour_conversions.nsmallest(3, 'conv_rate')\n",
    "\n",
    "print(\"\\nHOURLY PATTERNS:\")\n",
    "print(\"Best performing hours:\")\n",
    "for hour, row in best_hours.iterrows():\n",
    "    if not pd.isna(hour):\n",
    "        print(f\"  {int(hour):02d}:00 - {row['conv_rate']:.1f}% conversion ({int(row['sum'])}/{int(row['count'])} sessions)\")\n",
    "\n",
    "print(\"\\nWorst performing hours:\")\n",
    "for hour, row in worst_hours.iterrows():\n",
    "    if not pd.isna(hour):\n",
    "        print(f\"  {int(hour):02d}:00 - {row['conv_rate']:.1f}% conversion ({int(row['sum'])}/{int(row['count'])} sessions)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Cart Abandonment Impact\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CART ABANDONMENT IMPACT ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate abandonment metrics\n",
    "abandoned_sessions = touchpoints[touchpoints['cart_abandoned'] == True]\n",
    "total_abandoned = len(abandoned_sessions)\n",
    "total_lost_revenue = abandoned_sessions['cart_value'].sum()\n",
    "avg_abandoned_value = abandoned_sessions['cart_value'].mean()\n",
    "\n",
    "# Compare with converted carts\n",
    "converted_carts = touchpoints[(touchpoints['cart_additions'] > 0) & (touchpoints['conversion_status'] == True)]\n",
    "avg_converted_value = converted_carts['cart_value'].mean()\n",
    "\n",
    "print(f\"Abandoned Carts: {total_abandoned}\")\n",
    "print(f\"Total Lost Revenue: ${total_lost_revenue:,.2f}\")\n",
    "print(f\"Average Abandoned Cart Value: ${avg_abandoned_value:.2f}\")\n",
    "print(f\"Average Converted Cart Value: ${avg_converted_value:.2f}\")\n",
    "print(f\"Value Gap: ${avg_converted_value - avg_abandoned_value:.2f}\")\n",
    "\n",
    "# Abandonment by channel\n",
    "print(\"\\nAbandonment by Channel:\")\n",
    "channel_abandonment = touchpoints[touchpoints['cart_additions'] > 0].groupby('channel').agg({\n",
    "    'cart_abandoned': ['sum', 'mean']\n",
    "})\n",
    "channel_abandonment.columns = ['Abandoned_Carts', 'Abandonment_Rate']\n",
    "channel_abandonment['Abandonment_Rate'] = (channel_abandonment['Abandonment_Rate'] * 100).round(1)\n",
    "channel_abandonment = channel_abandonment.sort_values('Abandonment_Rate', ascending=False)\n",
    "print(channel_abandonment)\n",
    "\n",
    "# Project annual impact\n",
    "print(f\"\\nPROJECTED ANNUAL IMPACT:\")\n",
    "print(f\"   If we reduced cart abandonment by 50%: ${(total_lost_revenue * 0.5 * 365 / 30):,.0f} additional revenue/year\")\n",
    "print(f\"   If we achieved industry avg (25% abandonment): ${(total_lost_revenue * 0.75 * 365 / 30):,.0f} additional revenue/year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Project Work\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Diagnostic Data Analysis - Finding Hidden Friction Points\n",
    "Deep dive analysis to uncover the specific friction points causing the $31.2M annual revenue loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Plan for Diagnostic Analysis\n",
    "\n",
    "This section will identify the hidden friction points mentioned in the instructor synopsis:\n",
    "1. **The Conversion Illusion** - Why 70% conversion is misleading\n",
    "2. **Mobile Experience Crisis** - Quantify the mobile friction causing abandonment\n",
    "3. **Social Media Black Hole** - Why social channels have 0% conversion\n",
    "4. **Store Performance Anomalies** - Identify outlier stores and patterns\n",
    "5. **Journey Complexity Penalty** - How multi-touch journeys affect conversion\n",
    "6. **The 3 AM Miracle** - Time-based performance degradation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. THE CONVERSION ILLUSION - Revealing the truth behind 70% conversion\n",
    "print(\"=\"*60)\n",
    "print(\"DIAGNOSTIC FINDING #1: THE CONVERSION ILLUSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Break down conversion by channel type\n",
    "channel_breakdown = touchpoints.groupby('channel').agg({\n",
    "    'conversion_status': ['count', 'sum', 'mean']\n",
    "})\n",
    "channel_breakdown.columns = ['Total', 'Converted', 'Rate']\n",
    "channel_breakdown['Rate'] = channel_breakdown['Rate'] * 100\n",
    "channel_breakdown = channel_breakdown.sort_values('Rate', ascending=False)\n",
    "\n",
    "print(\"Conversion Rate by Channel:\")\n",
    "print(channel_breakdown)\n",
    "\n",
    "# Separate online vs offline\n",
    "online_channels = ['Instagram', 'Facebook', 'TikTok', 'Email', 'Google', 'Direct']\n",
    "offline_channels = ['Walk-In']\n",
    "\n",
    "online_touchpoints = touchpoints[touchpoints['channel'].isin(online_channels)]\n",
    "offline_touchpoints = touchpoints[touchpoints['channel'].isin(offline_channels)]\n",
    "\n",
    "online_conv = online_touchpoints['conversion_status'].mean() * 100\n",
    "offline_conv = offline_touchpoints['conversion_status'].mean() * 100\n",
    "\n",
    "print(f\"\\nTHE ILLUSION REVEALED:\")\n",
    "print(f\"   Overall Conversion: {conversion_rate:.1f}%\")\n",
    "print(f\"   Walk-in Store Conversion: {offline_conv:.1f}%\")\n",
    "print(f\"   Digital Channel Conversion: {online_conv:.1f}%\")\n",
    "print(f\"\\n The 70% rate is inflated by perfect store conversion!\")\n",
    "print(f\"   True digital experience converts at only {online_conv:.1f}%\")\n",
    "\n",
    "# Check if we're missing failed journeys\n",
    "customers_with_data = touchpoints['customer_id'].nunique()\n",
    "customers_who_converted = touchpoints[touchpoints['conversion_status'] == True]['customer_id'].nunique()\n",
    "print(f\"\\nDATA COMPLETENESS CHECK:\")\n",
    "print(f\"   Customers in touchpoint data: {customers_with_data}\")\n",
    "print(f\"   Customers who eventually converted: {customers_who_converted}\")\n",
    "print(f\"   Conversion rate among tracked customers: {(customers_who_converted/customers_with_data)*100:.1f}%\")\n",
    "print(f\"\\n WARNING: We may only be tracking customers who eventually purchase!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. MOBILE EXPERIENCE CRISIS - Quantifying mobile friction\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DIAGNOSTIC FINDING #2: MOBILE EXPERIENCE CRISIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Deep dive into mobile vs desktop behavior\n",
    "mobile_data = touchpoints[touchpoints['device_type'] == 'Mobile']\n",
    "desktop_data = touchpoints[touchpoints['device_type'] == 'Desktop']\n",
    "\n",
    "mobile_metrics = {\n",
    "    'Sessions': len(mobile_data),\n",
    "    'Conversion Rate': mobile_data['conversion_status'].mean() * 100,\n",
    "    'Avg Pages Viewed': mobile_data['pages_viewed'].mean(),\n",
    "    'Cart Addition Rate': (mobile_data['cart_additions'] > 0).mean() * 100,\n",
    "    'Avg Cart Value': mobile_data[mobile_data['cart_value'] > 0]['cart_value'].mean(),\n",
    "    'Abandonment Rate': mobile_data['cart_abandoned'].mean() * 100\n",
    "}\n",
    "\n",
    "desktop_metrics = {\n",
    "    'Sessions': len(desktop_data),\n",
    "    'Conversion Rate': desktop_data['conversion_status'].mean() * 100,\n",
    "    'Avg Pages Viewed': desktop_data['pages_viewed'].mean(),\n",
    "    'Cart Addition Rate': (desktop_data['cart_additions'] > 0).mean() * 100,\n",
    "    'Avg Cart Value': desktop_data[desktop_data['cart_value'] > 0]['cart_value'].mean(),\n",
    "    'Abandonment Rate': desktop_data['cart_abandoned'].mean() * 100\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame([mobile_metrics, desktop_metrics], index=['Mobile', 'Desktop']).T\n",
    "comparison_df['Gap'] = comparison_df['Desktop'] - comparison_df['Mobile']\n",
    "print(comparison_df.round(2))\n",
    "\n",
    "# Calculate revenue impact of mobile friction\n",
    "mobile_sessions_annual = len(mobile_data) * 365 / 30  # Extrapolate to annual\n",
    "current_mobile_conv = mobile_data['conversion_status'].mean()\n",
    "desktop_conv = desktop_data['conversion_status'].mean()\n",
    "mobile_avg_order = mobile_data[mobile_data['cart_value'] > 0]['cart_value'].mean()\n",
    "\n",
    "potential_additional_conversions = mobile_sessions_annual * (desktop_conv - current_mobile_conv)\n",
    "revenue_impact = potential_additional_conversions * mobile_avg_order\n",
    "\n",
    "print(f\"\\nMOBILE FRICTION IMPACT:\")\n",
    "print(f\"   Mobile underperforms desktop by {comparison_df.loc['Conversion Rate', 'Gap']:.1f} percentage points\")\n",
    "print(f\"   If mobile matched desktop conversion: {potential_additional_conversions:.0f} additional conversions/year\")\n",
    "print(f\"   Annual revenue loss from mobile friction: ${revenue_impact:,.0f}\")\n",
    "print(f\"\\n The case mentions 1.8-star app rating - this data confirms massive mobile issues!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. SOCIAL MEDIA BLACK HOLE - Why social doesn't convert\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DIAGNOSTIC FINDING #3: SOCIAL MEDIA BLACK HOLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Analyze social media performance\n",
    "social_channels = ['Instagram', 'Facebook', 'TikTok']\n",
    "social_touchpoints = touchpoints[touchpoints['channel'].isin(social_channels)]\n",
    "\n",
    "print(\"Social Media Channel Performance:\")\n",
    "for channel in social_channels:\n",
    "    channel_data = touchpoints[touchpoints['channel'] == channel]\n",
    "    if len(channel_data) > 0:\n",
    "        conv = channel_data['conversion_status'].sum()\n",
    "        total = len(channel_data)\n",
    "        rate = (conv / total) * 100\n",
    "        avg_pages = channel_data['pages_viewed'].mean()\n",
    "        cart_adds = channel_data['cart_additions'].sum()\n",
    "        \n",
    "        print(f\"\\n{channel}:\")\n",
    "        print(f\"  Sessions: {total}\")\n",
    "        print(f\"  Conversions: {conv}\")\n",
    "        print(f\"  Conversion Rate: {rate:.1f}%\")\n",
    "        print(f\"  Avg Pages Viewed: {avg_pages:.1f}\")\n",
    "        print(f\"  Total Cart Additions: {cart_adds}\")\n",
    "\n",
    "# Check if social drives awareness that converts elsewhere\n",
    "social_customers = social_touchpoints['customer_id'].unique()\n",
    "print(f\"\\n ATTRIBUTION ANALYSIS:\")\n",
    "print(f\"   Unique customers from social: {len(social_customers)}\")\n",
    "\n",
    "# Track these customers' full journey\n",
    "social_customer_journeys = touchpoints[touchpoints['customer_id'].isin(social_customers)]\n",
    "eventual_conversions = social_customer_journeys.groupby('customer_id')['conversion_status'].any().sum()\n",
    "\n",
    "print(f\"   Social customers who eventually converted (any channel): {eventual_conversions}\")\n",
    "print(f\"   True social-influenced conversion rate: {(eventual_conversions/len(social_customers))*100:.1f}%\")\n",
    "\n",
    "# Calculate lost attribution\n",
    "print(f\"\\n SOCIAL MEDIA ROI CRISIS:\")\n",
    "print(f\"   Case states $8.2M annual marketing spend\")\n",
    "print(f\"   Direct social conversions: {social_touchpoints['conversion_status'].sum()}\")\n",
    "print(f\"   Social gets 0% credit but influences {(eventual_conversions/len(social_customers))*100:.1f}% of conversions\")\n",
    "print(f\"   BROKEN ATTRIBUTION is hiding social's true value!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. STORE PERFORMANCE ANOMALIES - Finding outlier stores\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DIAGNOSTIC FINDING #4: STORE PERFORMANCE ANOMALIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Analyze store-level performance\n",
    "store_transactions = transactions[transactions['store_id'] != -1]  # Exclude online\n",
    "\n",
    "if len(store_transactions) > 0:\n",
    "    store_performance = store_transactions.groupby('store_id').agg({\n",
    "        'transaction_id': 'count',\n",
    "        'total_amount': ['mean', 'sum'],\n",
    "        'was_returned': 'mean'\n",
    "    }).round(2)\n",
    "    \n",
    "    store_performance.columns = ['Transactions', 'Avg_Order', 'Total_Revenue', 'Return_Rate']\n",
    "    store_performance['Return_Rate'] = store_performance['Return_Rate'] * 100\n",
    "    \n",
    "    # Merge with store information\n",
    "    store_performance = store_performance.reset_index()\n",
    "    store_performance['store_id'] = store_performance['store_id'].astype(int)\n",
    "    store_details = stores[['store_id', 'city', 'state']]\n",
    "    store_performance = store_performance.merge(store_details, on='store_id', how='left')\n",
    "    \n",
    "    store_performance = store_performance.sort_values('Avg_Order', ascending=False)\n",
    "    \n",
    "    print(\"Store Performance Rankings:\")\n",
    "    print(store_performance[['city', 'state', 'Transactions', 'Avg_Order', 'Return_Rate']].head(10))\n",
    "    \n",
    "    # Identify outliers\n",
    "    best_store = store_performance.iloc[0]\n",
    "    worst_store = store_performance.iloc[-1]\n",
    "    performance_gap = best_store['Avg_Order'] - worst_store['Avg_Order']\n",
    "    \n",
    "    print(f\"\\n STORE PERFORMANCE GAP:\")\n",
    "    print(f\"   Best Store: {best_store['city']} - ${best_store['Avg_Order']:.2f} avg order\")\n",
    "    print(f\"   Worst Store: {worst_store['city']} - ${worst_store['Avg_Order']:.2f} avg order\")\n",
    "    print(f\"   Performance Gap: ${performance_gap:.2f} ({(performance_gap/worst_store['Avg_Order']*100):.0f}% difference)\")\n",
    "    print(f\"\\n Like the Portland WhatsApp example in the case, top stores likely have workarounds!\")\n",
    "    \n",
    "    # Calculate impact of standardization\n",
    "    median_performance = store_performance['Avg_Order'].median()\n",
    "    underperforming = store_performance[store_performance['Avg_Order'] < median_performance]\n",
    "    potential_uplift = (median_performance - underperforming['Avg_Order'].mean()) * underperforming['Transactions'].sum()\n",
    "    \n",
    "    print(f\"\\n STANDARDIZATION OPPORTUNITY:\")\n",
    "    print(f\"   If all stores matched median performance: ${potential_uplift * 12:,.0f} additional revenue/year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. JOURNEY COMPLEXITY PENALTY - Multi-touch friction\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DIAGNOSTIC FINDING #5: JOURNEY COMPLEXITY PENALTY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Analyze conversion by journey complexity\n",
    "journey_analysis = customer_journey_metrics.copy()\n",
    "\n",
    "# Group by number of touchpoints\n",
    "touchpoint_buckets = pd.cut(journey_analysis['total_touchpoints'], \n",
    "                            bins=[0, 1, 2, 3, 100], \n",
    "                            labels=['1 Touch', '2 Touches', '3 Touches', '4+ Touches'])\n",
    "\n",
    "complexity_metrics = journey_analysis.groupby(touchpoint_buckets).agg({\n",
    "    'customer_id': 'count',\n",
    "    'converted': 'mean',\n",
    "    'is_multichannel': 'mean',\n",
    "    'is_multidevice': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "complexity_metrics.columns = ['Customers', 'Conversion_Rate', 'Multichannel_Rate', 'Multidevice_Rate']\n",
    "complexity_metrics['Conversion_Rate'] = complexity_metrics['Conversion_Rate'] * 100\n",
    "complexity_metrics['Multichannel_Rate'] = complexity_metrics['Multichannel_Rate'] * 100\n",
    "complexity_metrics['Multidevice_Rate'] = complexity_metrics['Multidevice_Rate'] * 100\n",
    "\n",
    "print(\"Conversion by Journey Complexity:\")\n",
    "print(complexity_metrics)\n",
    "\n",
    "# Analyze multichannel vs single channel\n",
    "multichannel_customers = journey_analysis[journey_analysis['is_multichannel'] == True]\n",
    "singlechannel_customers = journey_analysis[journey_analysis['is_multichannel'] == False]\n",
    "\n",
    "print(f\"\\nCHANNEL SWITCHING IMPACT:\")\n",
    "print(f\"   Single-channel customers: {len(singlechannel_customers)} ({len(singlechannel_customers)/len(journey_analysis)*100:.1f}%)\")\n",
    "print(f\"   Multi-channel customers: {len(multichannel_customers)} ({len(multichannel_customers)/len(journey_analysis)*100:.1f}%)\")\n",
    "\n",
    "if len(multichannel_customers) > 0:\n",
    "    print(f\"   Multi-channel avg touchpoints: {multichannel_customers['total_touchpoints'].mean():.1f}\")\n",
    "    print(f\"   Single-channel avg touchpoints: {singlechannel_customers['total_touchpoints'].mean():.1f}\")\n",
    "\n",
    "print(f\"\\nFRICTION ACCUMULATION:\")\n",
    "print(f\"   Only {journey_analysis['is_multichannel'].mean()*100:.1f}% of customers use multiple channels\")\n",
    "print(f\"   Only {journey_analysis['is_multidevice'].mean()*100:.1f}% use multiple devices\")\n",
    "print(f\"   Channel isolation is preventing omnichannel journeys!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. THE 3 AM MIRACLE - Time-based performance patterns\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DIAGNOSTIC FINDING #6: TIME-BASED PERFORMANCE DEGRADATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Detailed hourly analysis\n",
    "hourly_metrics = touchpoints.groupby('hour').agg({\n",
    "    'touchpoint_id': 'count',\n",
    "    'conversion_status': ['sum', 'mean'],\n",
    "    'pages_viewed': 'mean',\n",
    "    'cart_abandoned': 'sum'\n",
    "}).round(3)\n",
    "\n",
    "hourly_metrics.columns = ['Sessions', 'Conversions', 'Conv_Rate', 'Avg_Pages', 'Abandonments']\n",
    "hourly_metrics['Conv_Rate'] = hourly_metrics['Conv_Rate'] * 100\n",
    "hourly_metrics = hourly_metrics.sort_values('Conv_Rate', ascending=False)\n",
    "\n",
    "print(\"Top 5 Best Performing Hours:\")\n",
    "print(hourly_metrics.head())\n",
    "\n",
    "print(\"\\nBottom 5 Worst Performing Hours:\")\n",
    "print(hourly_metrics.tail())\n",
    "\n",
    "# Identify patterns\n",
    "business_hours = [9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
    "evening_hours = [19, 20, 21]\n",
    "night_hours = [22, 23, 0, 1, 2, 3, 4, 5]\n",
    "\n",
    "business_data = touchpoints[touchpoints['hour'].isin(business_hours)]\n",
    "evening_data = touchpoints[touchpoints['hour'].isin(evening_hours)]\n",
    "\n",
    "if len(business_data) > 0 and len(evening_data) > 0:\n",
    "    business_conv = business_data['conversion_status'].mean() * 100\n",
    "    evening_conv = evening_data['conversion_status'].mean() * 100\n",
    "    \n",
    "    print(f\"\\nTHE TIME-BASED PATTERN:\")\n",
    "    print(f\"   Business hours (9-18) conversion: {business_conv:.1f}%\")\n",
    "    print(f\"   Evening hours (19-21) conversion: {evening_conv:.1f}%\")\n",
    "    print(f\"   Performance drop: {business_conv - evening_conv:.1f} percentage points\")\n",
    "    \n",
    "    print(f\"\\nINSIGHT: Performance degrades during peak evening traffic!\")\n",
    "    print(f\"   This suggests site performance issues under load\")\n",
    "    print(f\"   The '3 AM Miracle' from the case - best performance when traffic is lowest!\")\n",
    "\n",
    "# Visualize the pattern\n",
    "if len(hourly_metrics) > 0:\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    hours = hourly_metrics.index.dropna()\n",
    "    conv_rates = hourly_metrics.loc[hours, 'Conv_Rate']\n",
    "    \n",
    "    plt.bar(hours, conv_rates, color=['green' if rate > 70 else 'orange' if rate > 50 else 'red' for rate in conv_rates])\n",
    "    plt.xlabel('Hour of Day', fontsize=12)\n",
    "    plt.ylabel('Conversion Rate (%)', fontsize=12)\n",
    "    plt.title('Conversion Rate by Hour - Revealing Performance Degradation', fontsize=14, fontweight='bold')\n",
    "    plt.axhline(y=conv_rates.mean(), color='blue', linestyle='--', alpha=0.5, label=f'Average: {conv_rates.mean():.1f}%')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Hidden Friction Points Discovered\n",
    "\n",
    "### Critical Findings:\n",
    "\n",
    "1. **The Conversion Illusion**: The 70.4% conversion rate is inflated by 100% walk-in store conversion. True digital conversion is much lower.\n",
    "\n",
    "2. **Mobile Experience Crisis**: Mobile underperforms desktop by 17.9 percentage points, causing significant revenue loss.\n",
    "\n",
    "3. **Social Media Black Hole**: Instagram and Facebook show 0% direct conversion despite $8.2M marketing spend. Attribution is completely broken.\n",
    "\n",
    "4. **Store Performance Anomalies**: 3.5x performance difference between best and worst stores suggests some locations have unofficial workarounds.\n",
    "\n",
    "5. **Journey Complexity Penalty**: Only 21% of customers are multi-channel, showing severe channel isolation.\n",
    "\n",
    "6. **Time-Based Degradation**: Evening hours show significantly worse conversion, suggesting performance issues under load.\n",
    "\n",
    "### Total Annual Revenue Impact: $31.2M\n",
    "\n",
    "This analysis confirms that StyleForward has created a fragmented experience where only the most determined customers complete purchases. The company needs immediate action on mobile optimization, attribution fixes, and channel integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "toc": {
   "base_numbering": 1
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
